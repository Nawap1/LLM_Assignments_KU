Assignment 5
Assignment 5.1: Model Comparison Report
Compare any two multimodal LLMs (e.g., CLIP, BLIP, Whisper). Write a short report
describing each modelâ€™s architecture, the types of input it uses (text, image, or audio),
and its main applications. Explain how the models handle cross-modal inputs. Include
one diagram and a simple comparison table. Add references if available.

Assignment 5.2: Multimodal Application Demo
Build a simple Python-based application using a pretrained multimodal model (e.g.,
BLIP, MobileVLM). The app should take an image and/or text input and generate a
relevant output like a caption or answer. Include a brief explanation of how the model
works, with sample input and output. Submit your code with a screenshot of the
working demo.