{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd96482d",
   "metadata": {},
   "source": [
    "# RAG Pipeline Implementation\n",
    "\n",
    "This notebook implements a basic Retrieval-Augmented Generation (RAG) pipeline using:\n",
    "- **crawl4ai**: For web crawling\n",
    "- **ChromaDB**: For vector storage\n",
    "- **LangChain**: For RAG orchestration\n",
    "- **Gemini**: As the language model\n",
    "- **Context7**: For additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3810b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: crawl4ai in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (0.6.3)\n",
      "Requirement already satisfied: chromadb in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (1.0.13)\n",
      "Requirement already satisfied: langchain in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: langchain-google-genai in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (2.1.5)\n",
      "Requirement already satisfied: langchain-community in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (0.3.26)\n",
      "Requirement already satisfied: sentence-transformers in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: nest-asyncio in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: requests in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (4.13.4)\n",
      "Requirement already satisfied: aiosqlite~=0.20 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (0.21.0)\n",
      "Requirement already satisfied: lxml~=5.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (5.4.0)\n",
      "Requirement already satisfied: litellm>=1.53.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (1.73.2)\n",
      "Requirement already satisfied: numpy<3,>=1.26.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (2.2.5)\n",
      "Requirement already satisfied: pillow~=10.4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (10.4.0)\n",
      "Requirement already satisfied: playwright>=1.49.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (1.53.0)\n",
      "Requirement already satisfied: python-dotenv~=1.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (1.1.0)\n",
      "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (1.2.0)\n",
      "Requirement already satisfied: xxhash~=3.4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (3.5.0)\n",
      "Requirement already satisfied: rank-bm25~=0.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (0.2.2)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (24.1.0)\n",
      "Requirement already satisfied: colorama~=0.4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (0.4.6)\n",
      "Requirement already satisfied: snowballstemmer~=2.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: pydantic>=2.10 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (2.11.4)\n",
      "Requirement already satisfied: pyOpenSSL>=24.3.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (25.1.0)\n",
      "Requirement already satisfied: psutil>=6.1.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (7.0.0)\n",
      "Requirement already satisfied: nltk>=3.9.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (3.9.1)\n",
      "Requirement already satisfied: rich>=13.9.4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (14.0.0)\n",
      "Requirement already satisfied: cssselect>=1.2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (1.3.0)\n",
      "Requirement already satisfied: httpx>=0.27.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (0.28.1)\n",
      "Requirement already satisfied: fake-useragent>=2.0.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.7 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (8.2.1)\n",
      "Requirement already satisfied: pyperclip>=1.8.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: chardet>=5.2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (5.2.0)\n",
      "Requirement already satisfied: aiohttp>=3.11.11 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (3.11.18)\n",
      "Requirement already satisfied: brotli>=1.1.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (1.1.0)\n",
      "Requirement already satisfied: humanize>=4.10.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from crawl4ai) (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Requirement already satisfied: build>=1.0.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (6.0.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (3.10.18)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain) (0.3.66)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain) (2.0.40)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from pydantic>=2.10->crawl4ai) (0.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain-google-genai) (0.6.18)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.0rc1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.40.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from aiohttp>=3.11.11->crawl4ai) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from sentence-transformers) (4.53.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: scipy in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from sentence-transformers) (0.31.1)\n",
      "Requirement already satisfied: filelock in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: pyproject_hooks in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: anyio in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from httpx>=0.27.2->crawl4ai) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from httpx>=0.27.2->crawl4ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.25.0)\n",
      "Requirement already satisfied: six>=1.9.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (8.6.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
      "Requirement already satisfied: openai>=1.68.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (1.78.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from litellm>=1.53.1->crawl4ai) (0.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.20 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
      "Requirement already satisfied: joblib in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from nltk>=3.9.1->crawl4ai) (1.5.0)\n",
      "Requirement already satisfied: coloredlogs in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: sympy in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (0.9.0)\n",
      "Requirement already satisfied: sniffio in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from anyio->httpx>=0.27.2->crawl4ai) (1.2.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: pyee<14,>=13 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from playwright>=1.49.0->crawl4ai) (13.0.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: cryptography<46,>=41.0.5 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from pyOpenSSL>=24.3.0->crawl4ai) (44.0.3)\n",
      "Requirement already satisfied: cffi>=1.12 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from cryptography<46,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
      "Requirement already satisfied: pycparser in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from cffi>=1.12->cryptography<46,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from rich>=13.9.4->crawl4ai) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from rich>=13.9.4->crawl4ai) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
      "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
      "Requirement already satisfied: networkx in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in f:\\programs\\anaconda3\\envs\\aibi\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install crawl4ai chromadb langchain langchain-google-genai langchain-community sentence-transformers nest-asyncio requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c17b33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import os\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a35d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "# Load environment variables from a .env file if present\n",
    "load_dotenv()\n",
    "WEBSITE_URL = \"https://python.langchain.com/docs/introduction/\"  # Example URL - replace with your target\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")  # Load from .env or environment\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables or .env file.\")\n",
    "\n",
    "# Set environment variable for Google API\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec238db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling website: https://python.langchain.com/docs/introduction/\n",
      "Advanced crawling failed: \n",
      "Advanced crawling failed: Advanced crawling returned no content\n",
      "üîÑ Falling back to simple crawling...\n",
      "‚úÖ Used simple crawling with requests/BeautifulSoup\n",
      "Crawled content length: 12218 characters\n",
      "First 500 characters:\n",
      "Introduction | ü¶úÔ∏èüîó LangChain Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Bui...\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Web Crawling with crawl4ai (Fixed for Jupyter)\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Apply nest_asyncio to handle asyncio in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def crawl_website_simple(url):\n",
    "    \"\"\"Simple web crawling using requests and BeautifulSoup as fallback\"\"\"\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "            \n",
    "        # Get text content\n",
    "        text = soup.get_text()\n",
    "        \n",
    "        # Clean up text\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = ' '.join(chunk for chunk in chunks if chunk)\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling with simple method: {e}\")\n",
    "        return None\n",
    "\n",
    "async def crawl_website_advanced(url):\n",
    "    \"\"\"Advanced crawling with crawl4ai (if it works)\"\"\"\n",
    "    try:\n",
    "        async with AsyncWebCrawler(verbose=False) as crawler:\n",
    "            result = await crawler.arun(url=url)\n",
    "            return result.markdown\n",
    "    except Exception as e:\n",
    "        print(f\"Advanced crawling failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Try advanced crawling first, fallback to simple\n",
    "print(f\"Crawling website: {WEBSITE_URL}\")\n",
    "\n",
    "try:\n",
    "    # Try the advanced method first\n",
    "    content = await crawl_website_advanced(WEBSITE_URL)\n",
    "    if not content:\n",
    "        raise Exception(\"Advanced crawling returned no content\")\n",
    "    print(\"‚úÖ Used advanced crawling with crawl4ai\")\n",
    "except Exception as e:\n",
    "    print(f\"Advanced crawling failed: {e}\")\n",
    "    print(\"üîÑ Falling back to simple crawling...\")\n",
    "    content = crawl_website_simple(WEBSITE_URL)\n",
    "    if content:\n",
    "        print(\"‚úÖ Used simple crawling with requests/BeautifulSoup\")\n",
    "    else:\n",
    "        print(\"‚ùå Both crawling methods failed\")\n",
    "\n",
    "if content:\n",
    "    print(f\"Crawled content length: {len(content)} characters\")\n",
    "    print(f\"First 500 characters:\\n{content[:500]}...\")\n",
    "else:\n",
    "    print(\"No content was crawled. Please check the URL or network connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed74c9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split content into 16 chunks\n",
      "First chunk preview:\n",
      "Introduction | ü¶úÔ∏èüîó LangChain Skip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Que...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Text Processing and Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split the content into chunks\n",
    "documents = text_splitter.split_text(content)\n",
    "print(f\"Split content into {len(documents)} chunks\")\n",
    "print(f\"First chunk preview:\\n{documents[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7092515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nawap Bastola\\AppData\\Local\\Temp\\ipykernel_22140\\795909416.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "f:\\Programs\\anaconda3\\envs\\aibi\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector store with 16 documents\n",
      "Vector store ready for retrieval\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Set up ChromaDB Vector Store\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# Create vector store using Chroma\n",
    "vectorstore = Chroma.from_texts(\n",
    "    texts=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(f\"Created vector store with {len(documents)} documents\")\n",
    "print(\"Vector store ready for retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb6c7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline setup complete!\n",
      "Ready to answer questions based on the crawled content\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Set up Gemini LLM and RAG Chain\n",
    "# Initialize Gemini model\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite-preview-06-17\", temperature=0.3)\n",
    "\n",
    "# Create retriever from vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Create RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"RAG pipeline setup complete!\")\n",
    "print(\"Ready to answer questions based on the crawled content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06155423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Query Function\n",
    "def query_rag(question):\n",
    "    \"\"\"Query the RAG pipeline with a question\"\"\"\n",
    "    try:\n",
    "        response = qa_chain({\"query\": question})\n",
    "        return {\n",
    "            \"answer\": response[\"result\"],\n",
    "            \"source_documents\": response[\"source_documents\"]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example usage function\n",
    "def ask_question(question):\n",
    "    \"\"\"Helper function to ask questions and display results\"\"\"\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    result = query_rag(question)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    else:\n",
    "        print(f\"Answer: {result['answer']}\")\n",
    "        print(f\"\\nBased on {len(result['source_documents'])} source documents\")\n",
    "    \n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6832ceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nawap Bastola\\AppData\\Local\\Temp\\ipykernel_22140\\2809447598.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"query\": question})\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RAG Pipeline with example questions:\n",
      "============================================================\n",
      "Question: What is the main topic of this website?\n",
      "--------------------------------------------------\n",
      "Answer: The main topic of this website is the **LangChain framework**.\n",
      "\n",
      "It provides documentation and resources for building applications with large language models, covering its various components like:\n",
      "\n",
      "*   **langchain-core**: Base abstractions.\n",
      "*   **Integration packages**: For specific models (e.g., OpenAI, Anthropic).\n",
      "*   **langchain**: Chains, agents, and retrieval strategies.\n",
      "*   **langchain-community**: Community-maintained integrations.\n",
      "*   **langgraph**: Orchestration framework.\n",
      "\n",
      "The site also offers guides, tutorials, and information about the LangChain ecosystem, including LangSmith and LangGraph.\n",
      "\n",
      "Based on 4 source documents\n",
      "==================================================\n",
      "Question: What are the key features mentioned?\n",
      "--------------------------------------------------\n",
      "Answer: The key features mentioned are:\n",
      "\n",
      "*   **langchain-core:** Base abstractions for chat models and other components.\n",
      "*   **Integration packages (e.g. langchain-openai, langchain-anthropic, etc.):** Lightweight packages for important integrations, co-maintained by the LangChain team and integration developers.\n",
      "*   **langchain:** Chains, agents, and retrieval strategies for an application's cognitive architecture.\n",
      "*   **langchain-community:** Third-party integrations maintained by the community.\n",
      "*   **langgraph:** An orchestration framework for combining LangChain components into production-ready applications, with features like persistence and streaming.\n",
      "\n",
      "Based on 4 source documents\n",
      "==================================================\n",
      "Question: How can I get started?\n",
      "--------------------------------------------------\n",
      "Answer: To get started with LangGraph, the best place to begin is the **tutorials section**. These are recommended for hands-on learners or if you have a specific project in mind.\n",
      "\n",
      "Here are some of the top tutorials to start with:\n",
      "\n",
      "*   **Build a Simple LLM Application**\n",
      "*   **Build a Chatbot**\n",
      "*   **Build an Agent**\n",
      "*   **Introduction to LangGraph**\n",
      "\n",
      "You can also explore the full list of LangChain tutorials and other LangGraph tutorials. For a deeper dive, consider the LangChain Academy course, **Introduction to LangGraph**.\n",
      "\n",
      "Additionally, LangGraph offers **how-to guides** for quick answers to specific tasks, and a **conceptual guide** that introduces key LangChain concepts.\n",
      "\n",
      "Based on 4 source documents\n",
      "==================================================\n",
      "Question: What are the benefits discussed?\n",
      "--------------------------------------------------\n",
      "Answer: The provided text doesn't discuss any specific benefits of using LangChain. It focuses on the architecture of the framework, its various libraries (langchain-core, integration packages, langchain, langchain-community, langgraph), and where to find documentation and tutorials.\n",
      "\n",
      "Based on 4 source documents\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Test the RAG Pipeline\n",
    "print(\"Testing RAG Pipeline with example questions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Example questions (customize based on your crawled content)\n",
    "example_questions = [\n",
    "    \"What is the main topic of this website?\",\n",
    "    \"What are the key features mentioned?\",\n",
    "    \"How can I get started?\",\n",
    "    \"What are the benefits discussed?\"\n",
    "]\n",
    "\n",
    "# Test with example questions\n",
    "for question in example_questions:\n",
    "    ask_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d47e4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This RAG pipeline implementation includes:\n",
    "\n",
    "1. **Web Crawling**: Using `crawl4ai` to extract content from websites\n",
    "2. **Text Processing**: Splitting content into manageable chunks\n",
    "3. **Vector Storage**: Using ChromaDB for efficient similarity search\n",
    "4. **LLM Integration**: Gemini model for generating responses\n",
    "5. **RAG Chain**: LangChain orchestration for retrieval and generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03928a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aibi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
