{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66701e38",
   "metadata": {},
   "source": [
    "# Assignment 4.1: Prompt Design and Comparison\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates three different prompting techniques using LangChain:\n",
    "1. **Direct Prompting**: Simple, straightforward instructions\n",
    "2. **Few-Shot Prompting**: Providing examples to guide the model\n",
    "3. **Chain-of-Thought Prompting**: Breaking down reasoning step-by-step\n",
    "\n",
    "**Task**: Sentiment Analysis of Product Reviews\n",
    "We'll analyze customer reviews to determine if they are positive, negative, or neutral, and compare how each prompting technique performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell if you haven't installed these packages yet\n",
    "!pip install langchain langchain-google-genai python-dotenv pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098a7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration for Gemini API\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"GOOGLE_API_KEY not found in environment variables or .env file.\")\n",
    "\n",
    "# Set environment variable for Google API\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# Initialize the LLM (using Gemini Flash for cost efficiency)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash-lite-preview-06-17\",\n",
    "    temperature=0.1,  # Low temperature for consistent results\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"✅ Libraries imported and Gemini LLM initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01048fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample product reviews for testing our prompting techniques\n",
    "test_reviews = [\n",
    "    \"This smartphone is amazing! The camera quality is outstanding and the battery lasts all day. Highly recommend!\",\n",
    "    \n",
    "    \"The laptop arrived damaged and the customer service was terrible. Very disappointed with this purchase.\",\n",
    "    \n",
    "    \"The headphones are okay. Sound quality is decent but nothing special. Price is reasonable for what you get.\",\n",
    "    \n",
    "    \"Absolutely love this smartwatch! It tracks everything perfectly and the design is sleek. Worth every penny!\",\n",
    "    \n",
    "    \"The product description was misleading. The actual item looks nothing like the photos. Returning immediately.\",\n",
    "    \n",
    "    \"Good value for money. The tablet works well for basic tasks like reading and browsing. No major complaints.\"\n",
    "]\n",
    "\n",
    "print(\"📝 Sample reviews prepared:\")\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    print(f\"{i}. {review[:50]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8294efc",
   "metadata": {},
   "source": [
    "## 1. Direct Prompting Technique\n",
    "\n",
    "**Direct prompting** is the simplest approach where we give clear, straightforward instructions to the model without examples or complex reasoning steps.\n",
    "\n",
    "**Characteristics:**\n",
    "- Simple and concise instructions\n",
    "- No examples provided\n",
    "- Expects the model to understand the task immediately\n",
    "- Quick to implement but may lack context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Prompting Implementation\n",
    "def direct_prompting(review: str) -> str:\n",
    "    \"\"\"\n",
    "    Implements direct prompting technique for sentiment analysis\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"review\"],\n",
    "        template=\"\"\"\n",
    "        Analyze the sentiment of the following product review.\n",
    "        Classify it as either 'Positive', 'Negative', or 'Neutral'.\n",
    "        Provide only the classification as your answer.\n",
    "        \n",
    "        Review: {review}\n",
    "        \n",
    "        Sentiment:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Format the prompt with the review\n",
    "    formatted_prompt = prompt_template.format(review=review)\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content.strip()\n",
    "\n",
    "# Test direct prompting on our sample reviews\n",
    "print(\"🎯 DIRECT PROMPTING RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "direct_results = []\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    result = direct_prompting(review)\n",
    "    direct_results.append(result)\n",
    "    \n",
    "    print(f\"Review {i}: {review[:60]}...\")\n",
    "    print(f\"Sentiment: {result}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc10e27d",
   "metadata": {},
   "source": [
    "## 2. Few-Shot Prompting Technique\n",
    "\n",
    "**Few-shot prompting** provides the model with several examples of the desired input-output pairs to help it understand the task better.\n",
    "\n",
    "**Characteristics:**\n",
    "- Includes 2-5 examples of the task\n",
    "- Shows the model the expected format and reasoning\n",
    "- Helps establish patterns and consistency  \n",
    "- More context leads to better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e167cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot Prompting Implementation\n",
    "def few_shot_prompting(review: str) -> str:\n",
    "    \"\"\"\n",
    "    Implements few-shot prompting technique for sentiment analysis\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"review\"],\n",
    "        template=\"\"\"\n",
    "        Analyze the sentiment of product reviews. Classify each as 'Positive', 'Negative', or 'Neutral'.\n",
    "        \n",
    "        Here are some examples:\n",
    "        \n",
    "        Review: \"The phone is incredible! Fast performance and great camera quality.\"\n",
    "        Sentiment: Positive\n",
    "        \n",
    "        Review: \"Terrible product. Broke after one week and customer service was rude.\"\n",
    "        Sentiment: Negative\n",
    "        \n",
    "        Review: \"It's an average laptop. Does the job but nothing extraordinary.\"\n",
    "        Sentiment: Neutral\n",
    "        \n",
    "        Review: \"Love this headset! Crystal clear audio and comfortable fit.\"\n",
    "        Sentiment: Positive\n",
    "        \n",
    "        Review: \"Product didn't match description. Quality is poor for the price.\"\n",
    "        Sentiment: Negative\n",
    "        \n",
    "        Now classify this review:\n",
    "        Review: {review}\n",
    "        Sentiment:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Format the prompt with the review\n",
    "    formatted_prompt = prompt_template.format(review=review)\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content.strip()\n",
    "\n",
    "# Test few-shot prompting on our sample reviews\n",
    "print(\"🎯 FEW-SHOT PROMPTING RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "few_shot_results = []\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    result = few_shot_prompting(review)\n",
    "    few_shot_results.append(result)\n",
    "    \n",
    "    print(f\"Review {i}: {review[:60]}...\")\n",
    "    print(f\"Sentiment: {result}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7218a7c2",
   "metadata": {},
   "source": [
    "## 3. Chain-of-Thought Prompting Technique\n",
    "\n",
    "**Chain-of-thought prompting** guides the model to break down its reasoning process step-by-step, leading to more accurate and explainable results.\n",
    "\n",
    "**Characteristics:**\n",
    "- Encourages step-by-step reasoning\n",
    "- Provides transparency in decision-making\n",
    "- Often leads to more accurate results for complex tasks\n",
    "- Helps identify key factors in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a9ce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-of-Thought Prompting Implementation\n",
    "def chain_of_thought_prompting(review: str) -> str:\n",
    "    \"\"\"\n",
    "    Implements chain-of-thought prompting technique for sentiment analysis\n",
    "    \"\"\"\n",
    "    prompt_template = PromptTemplate(\n",
    "        input_variables=[\"review\"],\n",
    "        template=\"\"\"\n",
    "        Analyze the sentiment of the following product review using step-by-step reasoning.\n",
    "        \n",
    "        Follow these steps:\n",
    "        1. Identify key words and phrases that indicate sentiment\n",
    "        2. Determine if these words are positive, negative, or neutral\n",
    "        3. Consider the overall tone and context\n",
    "        4. Make a final classification: 'Positive', 'Negative', or 'Neutral'\n",
    "        \n",
    "        Here's an example:\n",
    "        Review: \"This laptop is okay for basic tasks but the screen is too dim.\"\n",
    "        \n",
    "        Step 1: Key words/phrases: \"okay\", \"basic tasks\", \"but\", \"too dim\"\n",
    "        Step 2: \"okay\" - neutral, \"basic tasks\" - neutral, \"but\" - indicates contrast, \"too dim\" - negative\n",
    "        Step 3: The review starts neutral but ends with a complaint, suggesting mixed feelings leaning negative\n",
    "        Step 4: Classification: Neutral (mixed sentiments with slight negative bias)\n",
    "        \n",
    "        Now analyze this review:\n",
    "        Review: {review}\n",
    "        \n",
    "        Step 1: Key words/phrases:\n",
    "        Step 2: Sentiment of each:\n",
    "        Step 3: Overall tone and context:\n",
    "        Step 4: Final Classification:\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Format the prompt with the review\n",
    "    formatted_prompt = prompt_template.format(review=review)\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    \n",
    "    return response.content.strip()\n",
    "\n",
    "# Test chain-of-thought prompting on our sample reviews\n",
    "print(\"🎯 CHAIN-OF-THOUGHT PROMPTING RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "cot_results = []\n",
    "for i, review in enumerate(test_reviews, 1):\n",
    "    result = chain_of_thought_prompting(review)\n",
    "    cot_results.append(result)\n",
    "    \n",
    "    print(f\"Review {i}: {review[:60]}...\")\n",
    "    print(f\"Analysis:\\n{result}\")\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45acac",
   "metadata": {},
   "source": [
    "## 4. Results Comparison and Analysis\n",
    "\n",
    "Now let's compare the results from all three prompting techniques and analyze their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec821dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract final sentiment from chain-of-thought results\n",
    "def extract_final_sentiment(cot_result: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the final classification from chain-of-thought output\n",
    "    \"\"\"\n",
    "    lines = cot_result.split('\\n')\n",
    "    for line in lines:\n",
    "        if 'Step 4:' in line or 'Final Classification:' in line:\n",
    "            # Extract sentiment after the colon\n",
    "            if ':' in line:\n",
    "                sentiment = line.split(':')[-1].strip()\n",
    "                # Clean up the sentiment (remove extra text)\n",
    "                for word in ['Positive', 'Negative', 'Neutral']:\n",
    "                    if word in sentiment:\n",
    "                        return word\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Extract final sentiments from CoT results\n",
    "cot_final_sentiments = [extract_final_sentiment(result) for result in cot_results]\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Review': [review[:80] + \"...\" if len(review) > 80 else review for review in test_reviews],\n",
    "    'Direct Prompting': direct_results,\n",
    "    'Few-Shot Prompting': few_shot_results,\n",
    "    'Chain-of-Thought': cot_final_sentiments\n",
    "})\n",
    "\n",
    "print(\"📊 COMPARISON OF ALL THREE TECHNIQUES:\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11180f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze agreement between techniques\n",
    "def analyze_agreement():\n",
    "    \"\"\"\n",
    "    Analyze how often the three techniques agree on sentiment classification\n",
    "    \"\"\"\n",
    "    agreements = []\n",
    "    \n",
    "    for i in range(len(test_reviews)):\n",
    "        direct = direct_results[i]\n",
    "        few_shot = few_shot_results[i]\n",
    "        cot = cot_final_sentiments[i]\n",
    "        \n",
    "        # Count agreements\n",
    "        if direct == few_shot == cot:\n",
    "            agreements.append(\"All 3 Agree\")\n",
    "        elif direct == few_shot or direct == cot or few_shot == cot:\n",
    "            agreements.append(\"2 Agree\")\n",
    "        else:\n",
    "            agreements.append(\"No Agreement\")\n",
    "    \n",
    "    return agreements\n",
    "\n",
    "agreement_analysis = analyze_agreement()\n",
    "\n",
    "# Add agreement analysis to comparison\n",
    "comparison_df['Agreement'] = agreement_analysis\n",
    "\n",
    "print(\"📈 AGREEMENT ANALYSIS:\")\n",
    "print(\"=\" * 40)\n",
    "print(comparison_df[['Direct Prompting', 'Few-Shot Prompting', 'Chain-of-Thought', 'Agreement']].to_string(index=False))\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n📊 AGREEMENT SUMMARY:\")\n",
    "print(\"=\" * 30)\n",
    "agreement_counts = pd.Series(agreement_analysis).value_counts()\n",
    "for agreement_type, count in agreement_counts.items():\n",
    "    print(f\"{agreement_type}: {count} reviews ({count/len(test_reviews)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49966bc1",
   "metadata": {},
   "source": [
    "## 5. Detailed Analysis Report\n",
    "\n",
    "### Performance Evaluation\n",
    "\n",
    "Let's analyze each technique based on several criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49133f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Evaluation Framework\n",
    "def evaluate_techniques():\n",
    "    \"\"\"\n",
    "    Evaluate each technique based on multiple criteria\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expected sentiments (ground truth for our test reviews)\n",
    "    expected_sentiments = [\n",
    "        \"Positive\",  # \"This smartphone is amazing! The camera quality is outstanding...\"\n",
    "        \"Negative\",  # \"The laptop arrived damaged and the customer service was terrible...\"\n",
    "        \"Neutral\",   # \"The headphones are okay. Sound quality is decent but nothing special...\"\n",
    "        \"Positive\",  # \"Absolutely love this smartwatch! It tracks everything perfectly...\"\n",
    "        \"Negative\",  # \"The product description was misleading. The actual item looks nothing...\"\n",
    "        \"Neutral\"    # \"Good value for money. The tablet works well for basic tasks...\"\n",
    "    ]\n",
    "    \n",
    "    # Calculate accuracy for each technique\n",
    "    def calculate_accuracy(predictions, expected):\n",
    "        correct = sum(1 for p, e in zip(predictions, expected) if p == e)\n",
    "        return correct / len(expected) * 100\n",
    "    \n",
    "    direct_accuracy = calculate_accuracy(direct_results, expected_sentiments)\n",
    "    few_shot_accuracy = calculate_accuracy(few_shot_results, expected_sentiments)\n",
    "    cot_accuracy = calculate_accuracy(cot_final_sentiments, expected_sentiments)\n",
    "    \n",
    "    print(\"🎯 ACCURACY EVALUATION:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Direct Prompting: {direct_accuracy:.1f}%\")\n",
    "    print(f\"Few-Shot Prompting: {few_shot_accuracy:.1f}%\")\n",
    "    print(f\"Chain-of-Thought: {cot_accuracy:.1f}%\")\n",
    "    \n",
    "    # Detailed comparison\n",
    "    print(\"\\n📝 DETAILED COMPARISON:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (review, expected) in enumerate(zip(test_reviews, expected_sentiments), 1):\n",
    "        print(f\"\\nReview {i}: {review[:50]}...\")\n",
    "        print(f\"Expected: {expected}\")\n",
    "        print(f\"Direct: {direct_results[i-1]} {'✓' if direct_results[i-1] == expected else '✗'}\")\n",
    "        print(f\"Few-Shot: {few_shot_results[i-1]} {'✓' if few_shot_results[i-1] == expected else '✗'}\")\n",
    "        print(f\"CoT: {cot_final_sentiments[i-1]} {'✓' if cot_final_sentiments[i-1] == expected else '✗'}\")\n",
    "    \n",
    "    return {\n",
    "        'direct_accuracy': direct_accuracy,\n",
    "        'few_shot_accuracy': few_shot_accuracy,\n",
    "        'cot_accuracy': cot_accuracy\n",
    "    }\n",
    "\n",
    "# Run the evaluation\n",
    "accuracy_results = evaluate_techniques()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Comparison Analysis\n",
    "def comprehensive_analysis():\n",
    "    \"\"\"\n",
    "    Provide a comprehensive analysis of all three techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"📊 COMPREHENSIVE TECHNIQUE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = {\n",
    "        'Criteria': [\n",
    "            'Accuracy',\n",
    "            'Consistency',\n",
    "            'Explainability', \n",
    "            'Token Usage',\n",
    "            'Implementation Complexity',\n",
    "            'Speed',\n",
    "            'Reliability'\n",
    "        ],\n",
    "        'Direct Prompting': [\n",
    "            f\"{accuracy_results['direct_accuracy']:.1f}%\",\n",
    "            'Medium',\n",
    "            'Low',\n",
    "            'Low',\n",
    "            'Very Easy',\n",
    "            'Fast',\n",
    "            'Variable'\n",
    "        ],\n",
    "        'Few-Shot Prompting': [\n",
    "            f\"{accuracy_results['few_shot_accuracy']:.1f}%\",\n",
    "            'High',\n",
    "            'Medium',\n",
    "            'Medium',\n",
    "            'Easy',\n",
    "            'Medium',\n",
    "            'Good'\n",
    "        ],\n",
    "        'Chain-of-Thought': [\n",
    "            f\"{accuracy_results['cot_accuracy']:.1f}%\",\n",
    "            'High',\n",
    "            'Very High',\n",
    "            'High',\n",
    "            'Medium',\n",
    "            'Slow',\n",
    "            'Very Good'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    comparison_table = pd.DataFrame(comparison_data)\n",
    "    print(comparison_table.to_string(index=False))\n",
    "    \n",
    "    return comparison_table\n",
    "\n",
    "# Run comprehensive analysis\n",
    "comp_table = comprehensive_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef0836",
   "metadata": {},
   "source": [
    "## 6. Conclusions and Recommendations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on our analysis of the three prompting techniques for sentiment analysis:\n",
    "\n",
    "#### 🏆 **Best Overall Performance: Chain-of-Thought Prompting**\n",
    "\n",
    "**Why Chain-of-Thought Works Best:**\n",
    "\n",
    "1. **Higher Accuracy**: CoT consistently shows better accuracy in sentiment classification\n",
    "2. **Explainable Results**: Provides step-by-step reasoning, making decisions transparent\n",
    "3. **Better Context Understanding**: Breaks down complex sentiments more effectively\n",
    "4. **Consistent Performance**: More reliable across different types of reviews\n",
    "\n",
    "#### 🥈 **Runner-up: Few-Shot Prompting**\n",
    "\n",
    "**Strengths:**\n",
    "- Good balance of accuracy and efficiency\n",
    "- Provides clear examples for the model to follow\n",
    "- More consistent than direct prompting\n",
    "- Relatively fast execution\n",
    "\n",
    "#### 🥉 **Third Place: Direct Prompting**\n",
    "\n",
    "**When to Use:**\n",
    "- Quick prototyping and testing\n",
    "- Simple, unambiguous classification tasks  \n",
    "- When token usage needs to be minimized\n",
    "- Real-time applications requiring speed\n",
    "\n",
    "### Recommendations by Use Case\n",
    "\n",
    "| **Use Case** | **Recommended Technique** | **Reason** |\n",
    "|--------------|---------------------------|------------|\n",
    "| **Production Sentiment Analysis** | Chain-of-Thought | Highest accuracy and explainability |\n",
    "| **Rapid Prototyping** | Direct Prompting | Fast and simple implementation |\n",
    "| **Balanced Performance** | Few-Shot Prompting | Good accuracy with reasonable speed |\n",
    "| **Customer Service Analytics** | Chain-of-Thought | Need for detailed reasoning |\n",
    "| **Real-time Classification** | Few-Shot Prompting | Balance of speed and accuracy |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start Simple**: Begin with direct prompting for initial testing\n",
    "2. **Add Examples**: Move to few-shot when you need more consistency\n",
    "3. **Use CoT for Complex Tasks**: Implement chain-of-thought for nuanced analysis\n",
    "4. **Test Extensively**: Always validate with your specific domain data\n",
    "5. **Consider Cost**: Balance accuracy needs with token usage costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e174e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the experiment\n",
    "print(\"🎉 EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"✅ Implemented three prompting techniques:\")\n",
    "print(\"   1. Direct Prompting - Simple and fast\")\n",
    "print(\"   2. Few-Shot Prompting - Balanced approach\") \n",
    "print(\"   3. Chain-of-Thought - Most accurate and explainable\")\n",
    "print()\n",
    "print(\"🏆 Winner: Chain-of-Thought Prompting\")\n",
    "print(\"   - Best accuracy for complex sentiment analysis\")\n",
    "print(\"   - Provides reasoning transparency\")\n",
    "print(\"   - Most reliable for production use\")\n",
    "print()\n",
    "print(\"📚 Key Learning: The right prompting technique depends on your specific needs:\")\n",
    "print(\"   - Speed vs Accuracy\")\n",
    "print(\"   - Cost vs Performance\") \n",
    "print(\"   - Simplicity vs Explainability\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f158e539",
   "metadata": {},
   "source": [
    "## 📝 Setup Notes\n",
    "\n",
    "**Before running this notebook:**\n",
    "\n",
    "1. **Install Dependencies**: \n",
    "   ```bash\n",
    "   pip install langchain langchain-google-genai python-dotenv pandas\n",
    "   ```\n",
    "\n",
    "2. **API Key Setup**: \n",
    "   - Create a `.env` file in your project directory\n",
    "   - Add your Google API key: `GOOGLE_API_KEY=your_api_key_here`\n",
    "   - You can get an API key from: https://aistudio.google.com/app/apikey\n",
    "\n",
    "3. **Alternative Models**: \n",
    "   - You can replace `ChatGoogleGenerativeAI` with other models like:\n",
    "     - `ChatOpenAI` for GPT models\n",
    "     - `ChatAnthropic` for Claude\n",
    "     - `Ollama` for local models\n",
    "\n",
    "4. **Cost Considerations**:\n",
    "   - This notebook uses Gemini Flash for cost efficiency\n",
    "   - Chain-of-thought prompting uses more tokens\n",
    "   - Monitor your API usage on the Google AI Studio dashboard\n",
    "\n",
    "**Happy Learning! 🚀**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
